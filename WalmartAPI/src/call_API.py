
import os
import json
from datetime import datetime
from serpapi import GoogleSearch

# ƒê·ªçc API key t·ª´ bi·∫øn m√¥i tr∆∞·ªùng
API_KEY = os.getenv("API_KEY")

if not API_KEY:
    raise ValueError("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y API_KEY! H√£y t·∫°o file .env ho·∫∑c truy·ªÅn API key qua docker run -e API_KEY=...")

# ======== C√ÅC LO·∫†I H√ÄNG =========
QUERIES = [
    "coffee", "groceries", "furniture", "toys", "clothes",
    "beauty products", "headphones", "refrigerator", "microwave",
    "washing machine", "pet supplies", "baby products",
    "cleaning supplies", "kitchen appliances", "bedding",
    "mattresses", "shoes", "sporting goods", "books",
    "stationery", "office supplies", "outdoor furniture", "garden tools"
]
N_PAGES = 5  # s·ªë trang c·∫ßn crawl cho m·ªói lo·∫°i h√†ng

# ======== X√ÅC ƒê·ªäNH ƒê∆Ø·ªúNG D·∫™N G·ªêC =========
# BASE_DIR: th∆∞ m·ª•c cha c·ªßa src (project/)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data")


# ======== H√ÄM L·∫§Y D·ªÆ LI·ªÜU =========
def fetch_walmart_data(query, n_pages=1):
    results_all = []
    for page in range(1, n_pages + 1):
        params = {
            "engine": "walmart",
            "query": query,
            "api_key": API_KEY,
            "page": page
        }

        search = GoogleSearch(params)
        results = search.get_dict()

        if "error" in results:
            print(f"L·ªói v·ªõi query '{query}': {results['error']}")
            continue

        if "organic_results" in results:
            fetch_time = datetime.now().isoformat()
            for item in results["organic_results"]:
                item["query"] = query
                item["fetch_time"] = fetch_time
                results_all.append(item)
    return results_all


# ======== CH·∫†Y CH∆Ø∆†NG TR√åNH =========
if __name__ == "__main__":
    # 1. T·∫°o th∆∞ m·ª•c l∆∞u raw data
    raw_dir = os.path.join(DATA_DIR, "raw_data")
    os.makedirs(raw_dir, exist_ok=True)

    # 2. G·ªçi API v√† thu th·∫≠p d·ªØ li·ªáu
    all_data = []
    for q in QUERIES:
        print(f"ƒêang l·∫•y d·ªØ li·ªáu cho: {q} ...")
        products = fetch_walmart_data(q, N_PAGES)
        all_data.extend(products)

    # 3. L∆∞u file JSON v√†o data/raw_data/
    raw_path = os.path.join(raw_dir, "raw_data.json")
    with open(raw_path, "w", encoding="utf-8") as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu th√¥ v√†o: {raw_path}")
    print(f"üì¶ T·ªïng s·ªë s·∫£n ph·∫©m thu ƒë∆∞·ª£c: {len(all_data)}")
