"""
===================================================================
TI·ªÄN X·ª¨ L√ç D·ªÆ LI·ªÜU MARKETING - PHI√äN B·∫¢N HO√ÄN CH·ªàNH
===================================================================
K·∫øt h·ª£p:
1. Fix c·∫•u tr√∫c CSV (29 c·ªôt data ‚Üí 28 c·ªôt header)
2. Ti·ªÅn x·ª≠ l√≠ d·ªØ li·ªáu ƒë·ªânh cao v·ªõi ML algorithms
   - KNN Imputation (<5% missing)
   - Iterative Imputation/MICE (5-30% missing)
   - Median/Mode (>30% missing)
   - IQR Outlier Detection & Handling
   - Text Cleaning & Normalization
   - Feature Engineering
===================================================================
"""

import pandas as pd
import numpy as np
import csv
import json
import re
from datetime import datetime
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import KNNImputer, IterativeImputer
from scipy import stats

print("="*80)
print(" TI·ªÄN X·ª¨ L√ç D·ªÆ LI·ªÜU MARKETING - PHI√äN B·∫¢N HO√ÄN CH·ªàNH")
print("="*80)

# ==================== B∆Ø·ªöC 1: FIX C·∫§U TR√öC CSV ====================
print("\n B∆Ø·ªöC 1: ƒê·ªåC V√Ä FIX C·∫§U TR√öC CSV...")

rows = []
with open('marketing_data.csv', 'r', encoding='utf-8') as f:
    reader = csv.reader(f)
    header = next(reader)
    
    print(f"   Header: {len(header)} c·ªôt")
    
    for i, row in enumerate(reader):
        # Fix: Data c√≥ 29 c·ªôt, header c√≥ 28 c·ªôt ‚Üí b·ªè c·ªôt cu·ªëi
        if len(row) > len(header):
            row = row[:len(header)]
        elif len(row) < len(header):
            row = row + ['NA'] * (len(header) - len(row))
        
        rows.append(row)
        
        if (i+1) % 10000 == 0:
            print(f"   ƒêang ƒë·ªçc: {i+1:,} d√≤ng...")

print(f"‚úÖ ƒê·ªçc xong: {len(rows):,} d√≤ng")

# T·∫°o DataFrame
df = pd.DataFrame(rows, columns=header)
print(f"üìä DataFrame g·ªëc: {df.shape[0]:,} d√≤ng √ó {df.shape[1]} c·ªôt")

# ==================== B∆Ø·ªöC 2: PARSE NUMERIC COLUMNS ====================
print("\nüîß B∆Ø·ªöC 2: PARSE NUMERIC COLUMNS...")

def parse_numeric(value):
    """Parse numeric values, x·ª≠ l√≠ NA/null/currency"""
    if pd.isna(value) or value in ['NA', 'na', 'N/A', '', 'NULL', 'null']:
        return np.nan
    try:
        # Remove currency symbols v√† commas
        cleaned = str(value).replace(',', '').replace('$', '').replace('‚Ç¨', '').replace('¬£', '')
        return float(cleaned)
    except:
        return np.nan

# Parse c√°c c·ªôt numeric
numeric_columns = {
    'Price': 'Price',
    'Monthly Price': 'Monthly Price', 
    'Num Of Reviews': 'Num Of Reviews',
    'Average Rating': 'Average Rating',
    'Number Of Ratings': 'Number Of Ratings',
    'Five Star': 'Five Star',
    'Four Star': 'Four Star',
    'Three Star': 'Three Star',
    'Two Star': 'Two Star',
    'One Star': 'One Star'
}

for col in numeric_columns:
    if col in df.columns:
        df[col] = df[col].apply(parse_numeric)
        null_pct = df[col].isna().sum() / len(df) * 100
        print(f"   ‚úÖ {col}: {df[col].notna().sum():,} values ({100-null_pct:.2f}% complete)")

# ==================== B∆Ø·ªöC 3: CLEAN TEXT COLUMNS ====================
print("\nüìù B∆Ø·ªöC 3: CLEAN TEXT COLUMNS...")

def clean_text(text):
    """L√†m s·∫°ch text: normalize whitespace, handle NA"""
    if pd.isna(text) or text in ['NA', 'na', 'N/A', '', 'NULL', 'null']:
        return 'Unknown'
    
    text = str(text).strip()
    
    # Normalize whitespace
    text = re.sub(r'\s+', ' ', text)
    
    # Remove special characters ·ªü ƒë·∫ßu/cu·ªëi
    text = re.sub(r'^[^\w]+|[^\w]+$', '', text)
    
    return text if text else 'Unknown'

text_columns = ['Title', 'Manufacturer', 'Model Name', 'Carrier', 
                'Color Category', 'Internal Memory', 'Screen Size', 'Specifications']

for col in text_columns:
    if col in df.columns:
        df[col] = df[col].apply(clean_text)

print(f"‚úÖ L√†m s·∫°ch {len([c for c in text_columns if c in df.columns])} c·ªôt text")

# ==================== B∆Ø·ªöC 4: PARSE BOOLEAN & DATETIME ====================
print("\nüìÖ B∆Ø·ªöC 4: PARSE BOOLEAN & DATETIME...")

# Parse boolean columns
bool_map = {
    'true': True, 'True': True, 'TRUE': True, True: True, 't': True, 'T': True, '1': True,
    'false': False, 'False': False, 'FALSE': False, False: False, 'f': False, 'F': False, '0': False
}

bool_columns = ['Stock', 'Discontinued', 'Broken Link']
for col in bool_columns:
    if col in df.columns:
        df[col] = df[col].map(bool_map)
        df[col] = df[col].fillna(False)

# Parse datetime
if 'Crawl Timestamp' in df.columns:
    df['Crawl Timestamp'] = pd.to_datetime(df['Crawl Timestamp'], errors='coerce')
    df['crawl_year'] = df['Crawl Timestamp'].dt.year
    df['crawl_month'] = df['Crawl Timestamp'].dt.month
    df['crawl_day'] = df['Crawl Timestamp'].dt.day
    df['crawl_dayofweek'] = df['Crawl Timestamp'].dt.dayofweek

print("‚úÖ Parse boolean & datetime ho√†n th√†nh")

# ==================== B∆Ø·ªöC 5: PH√ÇN T√çCH MISSING VALUES ====================
print("\nüìä B∆Ø·ªöC 5: PH√ÇN T√çCH MISSING VALUES...")

missing_analysis = []
for col in df.columns:
    null_count = df[col].isna().sum()
    null_pct = null_count / len(df) * 100
    if null_pct > 0:
        missing_analysis.append({
            'column': col,
            'missing_count': null_count,
            'missing_pct': null_pct
        })

missing_df = pd.DataFrame(missing_analysis).sort_values('missing_pct', ascending=False)
print("\nTop 15 c·ªôt c√≥ missing values:")
print(missing_df.head(15).to_string(index=False))

# ==================== B∆Ø·ªöC 6: IMPUTE MISSING VALUES ====================
print("\nüéØ B∆Ø·ªöC 6: IMPUTE MISSING VALUES V·ªöI ML ALGORITHMS...")

# X√°c ƒë·ªãnh c·ªôt c·∫ßn impute (numeric columns v·ªõi 0-95% missing)
impute_cols = []
for col in ['Price', 'Monthly Price', 'Average Rating', 'Num Of Reviews', 
            'Number Of Ratings', 'Five Star', 'Four Star', 'Three Star', 
            'Two Star', 'One Star']:
    if col in df.columns:
        missing_pct = df[col].isna().sum() / len(df) * 100
        if 0 < missing_pct < 95:
            impute_cols.append(col)

print(f"\nC·ªôt s·∫Ω impute: {impute_cols}")

def smart_impute_numeric(df, columns):
    """
    Impute numeric columns v·ªõi strategy th√¥ng minh:
    - <5% missing: KNN Imputation
    - 5-30% missing: Iterative Imputation (MICE)
    - >30% missing: Median Imputation
    """
    df_result = df.copy()
    
    for col in columns:
        missing_pct = df[col].isna().sum() / len(df) * 100
        
        if missing_pct == 0:
            continue
            
        print(f"\n   ‚Ä¢ {col} ({missing_pct:.2f}% missing):")
        
        if missing_pct < 5:
            # KNN Imputation
            print(f"      ‚Üí S·ª≠ d·ª•ng KNN Imputation (n_neighbors=5)")
            
            # T√¨m c√°c c·ªôt t∆∞∆°ng quan ƒë·ªÉ l√†m features
            numeric_df = df[columns].select_dtypes(include=[np.number])
            
            if col in numeric_df.columns and numeric_df[col].notna().sum() > 0:
                # T√≠nh correlation
                corr = numeric_df.corr()[col].abs().sort_values(ascending=False)
                # L·∫•y top 6 c·ªôt (bao g·ªìm ch√≠nh n√≥)
                feature_cols = [c for c in corr.head(6).index.tolist() if c in df.columns]
                
                if len(feature_cols) > 1:
                    imputer = KNNImputer(n_neighbors=min(5, df[feature_cols].notna().all(axis=1).sum()), 
                                        weights='distance')
                    df_result[feature_cols] = imputer.fit_transform(df[feature_cols])
                else:
                    # Fallback to median
                    df_result[col] = df_result[col].fillna(df[col].median())
            
        elif missing_pct < 30:
            # Iterative Imputation (MICE)
            print(f"      ‚Üí S·ª≠ d·ª•ng Iterative Imputation (MICE, max_iter=10)")
            
            numeric_df = df[columns].select_dtypes(include=[np.number])
            
            if len(numeric_df.columns) > 1:
                imputer = IterativeImputer(max_iter=10, random_state=42)
                df_result[numeric_df.columns] = imputer.fit_transform(numeric_df)
            else:
                df_result[col] = df_result[col].fillna(df[col].median())
                
        else:
            # Median Imputation
            print(f"      ‚Üí S·ª≠ d·ª•ng Median Imputation")
            median_val = df[col].median()
            if pd.isna(median_val):
                median_val = 0
            df_result[col] = df_result[col].fillna(median_val)
    
    return df_result

if impute_cols:
    df = smart_impute_numeric(df, impute_cols)
    print("\n‚úÖ Imputation ho√†n th√†nh")

# ==================== B∆Ø·ªöC 7: X·ª¨ L√ç OUTLIERS ====================
print("\nüîç B∆Ø·ªöC 7: X·ª¨ L√ç OUTLIERS V·ªöI IQR METHOD...")

def detect_and_handle_outliers(df, column):
    """
    Ph√°t hi·ªán v√† x·ª≠ l√≠ outliers v·ªõi IQR method
    Strategy:
    - <5% outliers: Winsorization (1st-99th percentile)
    - 5-15% outliers: IQR Capping
    - >15% outliers: Gi·ªØ nguy√™n
    """
    if df[column].dtype not in ['float64', 'int64']:
        return df, 0
    
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = ((df[column] < lower_bound) | (df[column] > upper_bound))
    outlier_count = outliers.sum()
    outlier_pct = outlier_count / len(df) * 100
    
    if outlier_count == 0:
        return df, 0
    
    print(f"   ‚Ä¢ {column}: {outlier_count:,} outliers ({outlier_pct:.2f}%)")
    
    if outlier_pct < 5:
        # Winsorization
        p1 = df[column].quantile(0.01)
        p99 = df[column].quantile(0.99)
        df[column] = df[column].clip(lower=p1, upper=p99)
        print(f"      ‚Üí Winsorized: [{p1:.2f}, {p99:.2f}]")
    elif outlier_pct < 15:
        # IQR Capping
        df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
        print(f"      ‚Üí IQR Capped: [{lower_bound:.2f}, {upper_bound:.2f}]")
    else:
        print(f"      ‚Üí Gi·ªØ nguy√™n (>15% outliers)")
    
    return df, outlier_count

outlier_cols = ['Price', 'Monthly Price', 'Average Rating', 'Num Of Reviews', 
                'Number Of Ratings']
total_outliers = 0

for col in outlier_cols:
    if col in df.columns:
        df, count = detect_and_handle_outliers(df, col)
        total_outliers += count

print(f"\n‚úÖ X·ª≠ l√≠ {total_outliers:,} outliers t·ªïng c·ªông")

# ==================== B∆Ø·ªöC 8: FEATURE ENGINEERING ====================
print("\nüé® B∆Ø·ªöC 8: FEATURE ENGINEERING...")

features_created = []

# 1. Total star ratings
if all(col in df.columns for col in ['Five Star', 'Four Star', 'Three Star', 'Two Star', 'One Star']):
    df['total_star_ratings'] = (
        df['Five Star'] + df['Four Star'] + 
        df['Three Star'] + df['Two Star'] + df['One Star']
    )
    features_created.append('total_star_ratings')

# 2. Has reviews flag
if 'Num Of Reviews' in df.columns:
    df['has_reviews'] = (df['Num Of Reviews'] > 0).astype(int)
    features_created.append('has_reviews')

# 3. Price range category
if 'Price' in df.columns:
    df['price_range'] = pd.cut(
        df['Price'],
        bins=[0, 50, 100, 200, 500, float('inf')],
        labels=['Budget', 'Mid', 'Premium', 'High-end', 'Luxury']
    )
    features_created.append('price_range')

# 4. Rating quality
if 'Average Rating' in df.columns:
    df['rating_quality'] = pd.cut(
        df['Average Rating'],
        bins=[0, 2, 3, 4, 5],
        labels=['Poor', 'Fair', 'Good', 'Excellent']
    )
    features_created.append('rating_quality')

# 5. Review density (reviews per rating)
if 'Num Of Reviews' in df.columns and 'Number Of Ratings' in df.columns:
    df['review_density'] = df['Num Of Reviews'] / (df['Number Of Ratings'] + 1)
    features_created.append('review_density')

# 6. Price per rating point
if 'Price' in df.columns and 'Average Rating' in df.columns:
    df['price_per_rating'] = df['Price'] / (df['Average Rating'] + 0.1)
    features_created.append('price_per_rating')

print(f"‚úÖ T·∫°o {len(features_created)} features m·ªõi:")
for feat in features_created:
    print(f"   - {feat}")

# ==================== B∆Ø·ªöC 9: DROP LOW-VALUE COLUMNS ====================
print("\nüóëÔ∏è  B∆Ø·ªöC 9: DROP C√ÅC C·ªòT KH√îNG GI√Å TR·ªä...")

drop_candidates = []
for col in df.columns:
    null_pct = df[col].isna().sum() / len(df) * 100
    unique_count = df[col].nunique()
    
    should_drop = False
    reason = ""
    
    # Drop n·∫øu >95% missing
    if null_pct > 95:
        should_drop = True
        reason = f"{null_pct:.1f}% missing"
    # Drop n·∫øu ch·ªâ 1 gi√° tr·ªã unique
    elif unique_count == 1:
        should_drop = True
        reason = "Only 1 unique value"
    # Drop ID columns kh√¥ng c·∫ßn thi·∫øt
    elif col in ['Uniq Id', 'Pageurl'] and unique_count > len(df) * 0.95:
        should_drop = True
        reason = "ID column kh√¥ng c·∫ßn cho analysis"
    
    if should_drop:
        drop_candidates.append({
            'column': col,
            'reason': reason,
            'null_pct': null_pct,
            'unique': unique_count
        })

if drop_candidates:
    drop_cols = [item['column'] for item in drop_candidates]
    print(f"   Dropping {len(drop_cols)} columns:")
    for item in drop_candidates:
        print(f"      - {item['column']}: {item['reason']}")
    
    df = df.drop(columns=drop_cols)
else:
    print("   ‚úÖ Kh√¥ng c√≥ c·ªôt n√†o c·∫ßn drop")

# ==================== B∆Ø·ªöC 10: NORMALIZE COLUMN NAMES ====================
print("\nüìã B∆Ø·ªöC 10: NORMALIZE COLUMN NAMES...")

df.columns = (df.columns
              .str.strip()
              .str.lower()
              .str.replace(' ', '_')
              .str.replace(r'[^\w]', '', regex=True))

print(f"‚úÖ Normalize {len(df.columns)} column names")

# ==================== B∆Ø·ªöC 11: L∆ØU K·∫æT QU·∫¢ ====================
print("\nüíæ B∆Ø·ªöC 11: L∆ØU K·∫æT QU·∫¢...")

# L∆∞u clean data
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_file = f'marketing_data_clean_{timestamp}.csv'
df.to_csv(output_file, index=False, encoding='utf-8-sig')
print(f"‚úÖ L∆∞u file: {output_file}")

# T·∫°o schema JSON
schema = {
    'filename': output_file,
    'created_at': datetime.now().isoformat(),
    'original_rows': 29991,
    'final_rows': len(df),
    'original_columns': 28,
    'final_columns': len(df.columns),
    'data_retention_rate': f"{len(df)/29991*100:.2f}%",
    'column_info': {}
}

for col in df.columns:
    col_info = {
        'dtype': str(df[col].dtype),
        'null_count': int(df[col].isna().sum()),
        'null_pct': float(df[col].isna().sum() / len(df) * 100),
        'unique_count': int(df[col].nunique())
    }
    
    # Add sample values
    if df[col].dtype in ['object', 'category', 'bool']:
        col_info['sample_values'] = df[col].dropna().head(5).tolist()
    elif 'datetime' in str(df[col].dtype):
        col_info['min'] = str(df[col].min()) if pd.notna(df[col].min()) else None
        col_info['max'] = str(df[col].max()) if pd.notna(df[col].max()) else None
    else:
        try:
            col_info['min'] = float(df[col].min()) if pd.notna(df[col].min()) else None
            col_info['max'] = float(df[col].max()) if pd.notna(df[col].max()) else None
            col_info['mean'] = float(df[col].mean()) if pd.notna(df[col].mean()) else None
        except:
            col_info['sample_values'] = df[col].dropna().head(5).tolist()
    
    schema['column_info'][col] = col_info

with open('marketing_data_schema.json', 'w', encoding='utf-8') as f:
    json.dump(schema, f, indent=2, ensure_ascii=False)
print("‚úÖ L∆∞u schema: marketing_data_schema.json")

# T·∫°o b√°o c√°o chi ti·∫øt
report = f"""
{'='*80}
B√ÅO C√ÅO TI·ªÄN X·ª¨ L√ç D·ªÆ LI·ªÜU MARKETING - PHI√äN B·∫¢N HO√ÄN CH·ªàNH
{'='*80}
Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

üìä T·ªîNG QUAN:
   Input:  29,991 d√≤ng √ó 28 c·ªôt
   Output: {len(df):,} d√≤ng √ó {len(df.columns)} c·ªôt
   T·ªâ l·ªá gi·ªØ l·∫°i: {len(df)/29991*100:.2f}%

üîß C√ÅC B∆Ø·ªöC ƒê√É TH·ª∞C HI·ªÜN:
   1. ‚úÖ Fix c·∫•u tr√∫c CSV (29 c·ªôt data ‚Üí 28 c·ªôt header)
   2. ‚úÖ Parse numeric columns: Price, Ratings, Reviews
   3. ‚úÖ Clean text columns: Title, Manufacturer, Model Name...
   4. ‚úÖ Parse boolean & datetime columns
   5. ‚úÖ Ph√¢n t√≠ch missing values
   6. ‚úÖ Impute missing values:
      - KNN Imputation (<5% missing)
      - Iterative/MICE (5-30% missing)
      - Median (>30% missing)
   7. ‚úÖ X·ª≠ l√≠ outliers v·ªõi IQR method
   8. ‚úÖ Feature engineering: {len(features_created)} features m·ªõi
   9. ‚úÖ Drop low-value columns
   10. ‚úÖ Normalize column names
   11. ‚úÖ Export CSV + JSON schema

üìà C√ÅC C·ªòT QUAN TR·ªåNG:
"""

important_cols = ['price', 'average_rating', 'num_of_reviews', 
                  'five_star', 'four_star', 'three_star', 'two_star', 'one_star',
                  'model_name', 'manufacturer', 'title']

for col in important_cols:
    if col in df.columns:
        null_pct = df[col].isna().sum() / len(df) * 100
        unique_count = df[col].nunique()
        report += f"   - {col}: {df[col].dtype}, {null_pct:.2f}% missing, {unique_count:,} unique values\n"

report += f"\nüé® FEATURES M·ªöI:\n"
for feat in features_created:
    if feat in df.columns:
        report += f"   - {feat}\n"

report += f"\n{'='*80}\n"
report += "‚úÖ TI·ªÄN X·ª¨ L√ç HO√ÄN T·∫§T!\n"
report += f"{'='*80}\n"

with open('data_cleaning_report.txt', 'w', encoding='utf-8') as f:
    f.write(report)

print(report)

print("\n" + "="*80)
print("üéâ HO√ÄN TH√ÄNH TO√ÄN B·ªò QUY TR√åNH!")
print("üìÅ Files output:")
print(f"   - {output_file}")
print("   - marketing_data_schema.json")
print("   - data_cleaning_report.txt")
print("="*80)
